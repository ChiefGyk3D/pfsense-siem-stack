# Logstash Pipeline for Suricata EVE JSON Logs
#
# Purpose: Receives Suricata eve.json events via UDP from pfSense forwarder,
#          parses JSON, nests under suricata.eve.* namespace, and indexes to OpenSearch
#
# Data Flow:
#   pfSense (Suricata) → Forwarder (UDP) → Logstash (this pipeline) → OpenSearch
#
# Deployed to: /etc/logstash/conf.d/suricata.conf
# Restart after changes: sudo systemctl restart logstash

# ========================================================================
# INPUT: UDP Listener
# ========================================================================
input {
  udp {
    port => 5140                      # Listen on UDP 5140 (forwarder sends here)
    codec => plain                    # Receive raw text (JSON strings)
    buffer_size => 65536              # 64KB buffer per datagram (UDP limit)
    receive_buffer_bytes => 33554432  # 32MB OS-level buffer (handles bursts)
  }
}

# ========================================================================
# FILTER: Parse JSON and Nest Under suricata.eve.*
# ========================================================================
filter {
  # Step 1: Parse incoming JSON from message or event.original field
  # --------------------------------------------------------------
  # Forwarder sends JSON as plain text. Try parsing from standard fields.
  
  if [event][original] {
    # ECS format (some forwarders use this)
    json {
      source => "[event][original]"
      tag_on_failure => ["_jsonparsefailure"]
    }
  } else if [message] {
    # Standard Logstash field (our forwarder uses this)
    json {
      source => "message"
      tag_on_failure => ["_jsonparsefailure"]
    }
  }
  
  # Step 2: Nest parsed JSON under suricata.eve.* namespace
  # --------------------------------------------------------
  # Why? Consistent field paths for Grafana dashboards. All Suricata data
  # lives under suricata.eve.event_type, suricata.eve.src_ip, etc.
  # This makes queries predictable and avoids field name collisions.
  
  if "_jsonparsefailure" not in [tags] {
    ruby {
      code => "
        # Get raw JSON string (from message or event.original)
        raw = event.get('message')
        if raw.nil? && event.get('[event][original]')
          raw = event.get('[event][original]')
        end
        
        # Parse JSON and nest under [suricata][eve]
        if raw
          require 'json'
          begin
            parsed = JSON.parse(raw)
            event.set('[suricata][eve]', parsed)
          rescue => e
            # If parsing fails, log error (optional: add to tags)
            # event.set('ruby_exception', e.message)
          end
        end
      "
    }
    
    # Step 3: Extract timestamp from nested field
    # --------------------------------------------
    # Suricata includes ISO8601 timestamp. Use it for @timestamp field.
    # This ensures events are indexed with correct time, not Logstash receive time.
    
    if [suricata][eve][timestamp] {
      date {
        match => [ "[suricata][eve][timestamp]", "ISO8601" ]
        target => "@timestamp"
        tag_on_failure => ["_dateparsefailure"]
      }
    }
    
    # Step 4: Clean up raw message fields
    # ------------------------------------
    # Remove redundant fields now that data is nested under suricata.eve.*
    # Saves storage space and avoids duplication.
    
    mutate {
      remove_field => ["message", "[event][original]"]
    }
  }
  
  # Step 5: Add metadata for index naming
  # --------------------------------------
  # Index name will be: suricata-YYYY.MM.DD (daily indices)
  # Metadata field is not indexed, only used for index name templating.
  
  mutate {
    add_field => { "[@metadata][index_date]" => "%{+YYYY.MM.dd}" }
  }
}

# ========================================================================
# OUTPUT: Send to OpenSearch
# ========================================================================
output {
  opensearch {
    hosts => ["http://localhost:9200"]           # OpenSearch endpoint
    index => "suricata-%{[@metadata][index_date]}" # Daily indices: suricata-2025.11.27
    ssl => false                                  # No SSL for localhost (use true for remote)
    ssl_certificate_verification => false         # Disable cert check (for self-signed certs)
    
    # Optional: Add authentication if OpenSearch has security enabled
    # user => "admin"
    # password => "admin"
    
    # Optional: Batch settings for performance
    # flush_size => 500           # Bulk size (events per batch)
    # idle_flush_time => 1        # Flush interval (seconds)
  }
  
  # Optional: Debug output (uncomment to log events to console)
  # stdout {
  #   codec => rubydebug
  # }
}

# ========================================================================
# CONFIGURATION NOTES
# ========================================================================
#
# 1. Nested Structure:
#    All Suricata fields are under suricata.eve.* (e.g., suricata.eve.src_ip)
#    This matches the Grafana dashboard queries. Do NOT flatten to root level.
#
# 2. GeoIP Enrichment:
#    GeoIP is added by the forwarder BEFORE sending to Logstash. No GeoIP
#    filter needed here. Look for suricata.eve.geoip_src.location field.
#
# 3. Index Template:
#    OpenSearch must have proper index template for suricata-* indices.
#    Run: ./scripts/install-opensearch-config.sh to create template.
#    Template defines field mappings (geo_point, keyword, nested, etc.)
#
# 4. Performance Tuning:
#    - Increase receive_buffer_bytes if dropping UDP packets (check dmesg)
#    - Increase Logstash heap: /etc/logstash/jvm.options (-Xms2g -Xmx2g)
#    - Monitor pipeline with: curl localhost:9600/_node/stats/pipelines
#
# 5. Testing:
#    Send test event:
#      echo '{"timestamp":"2025-11-27T12:00:00.000000-0500","event_type":"test","src_ip":"1.2.3.4"}' | nc -u localhost 5140
#    Check in OpenSearch:
#      curl -s localhost:9200/suricata-*/_search?q=event_type:test | jq
#
# 6. Troubleshooting:
#    - Check Logstash logs: tail -f /var/log/logstash/logstash-plain.log
#    - Verify receiving data: tcpdump -i any -n udp port 5140
#    - Test pipeline: /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/suricata.conf --config.test_and_exit
#
# ========================================================================
